# TerminologÃ­a y conceptos esenciales

## Datos
- **Dataset:** colecciÃ³n de ejemplos etiquetados o no etiquetados.
- **Features (caracterÃ­sticas):** variables de entrada; pueden ser numÃ©ricas, categÃ³ricas, texto o imÃ¡genes.
- **Etiquetas (labels):** valores objetivo en tareas supervisadas.
- **Split:** divisiÃ³n en `train`, `valid` y `test` para estimar generalizaciÃ³n.
- **Leakage:** cuando informaciÃ³n del futuro o de la etiqueta se filtra en el entrenamiento.

### Mini ejercicio guiado
Completa esta tabla en tu cuaderno para el dataset que elijas:

| Dataset | Tipo de problema | Features clave | Etiqueta | Riesgo de leakage |
|---------|------------------|----------------|----------|-------------------|
|         |                  |                |          |                   |

## Modelos
- **ParÃ¡metros:** valores que el algoritmo aprende (pesos de una red, coeficientes de una regresiÃ³n lineal).
- **HiperparÃ¡metros:** configuraciones externas al entrenamiento (learning rate, profundidad de Ã¡rbol, nÃºmero de Ã©pocas).
- **RegularizaciÃ³n:** tÃ©cnicas para evitar overfitting (L1/L2, dropout, early stopping).
- **Capacidad del modelo:** quÃ© tan flexible es para ajustarse a los datos; demasiada capacidad lleva a overfitting.

> ğŸ¯ **Reto rÃ¡pido**: seÃ±ala un hiperparÃ¡metro que impacte la capacidad de tu modelo elegido y describe cÃ³mo lo ajustarÃ­as.

## Entrenamiento y evaluaciÃ³n
- **FunciÃ³n de pÃ©rdida:** quÃ© intenta minimizar el modelo (ej. entropÃ­a cruzada, MSE).
- **MÃ©trica:** cÃ³mo medimos Ã©xito; puede diferir de la pÃ©rdida (accuracy, F1, ROC-AUC, BLEU).
- **ValidaciÃ³n cruzada (k-fold):** reutilizar datos para evaluar estabilidad del modelo.
- **Seed:** semilla para reproducibilidad.
- **Curvas de aprendizaje:** grÃ¡ficas de pÃ©rdida/mÃ©trica vs Ã©pocas para detectar underfitting/overfitting.

### Checklist de evaluaciÃ³n
- [ ] Â¿La mÃ©trica elegida penaliza los errores que mÃ¡s importan al negocio?
- [ ] Â¿Comparaste el modelo contra una lÃ­nea base simple?
- [ ] Â¿Mediste varianza con validaciÃ³n cruzada o mÃºltiples seeds?

## Inferencia y despliegue
- **Latencia:** tiempo de respuesta en producciÃ³n.
- **Throughput:** volumen de peticiones procesadas.
- **Monitoreo:** alertas por deriva de datos, caÃ­da de mÃ©tricas o errores.
- **Guardrails:** reglas simples que evitan respuestas inseguras o fuera de polÃ­tica.

### Diagrama de despliegue mÃ­nimo
```
Cliente -> API -> Modelo -> Logs/MÃ©tricas -> Monitoreo
             |--> Reintentos/Timeouts
```

## Ejercicio prÃ¡ctico
1. Elige un conjunto de datos abierto (ej. Iris, Titanic, MNIST) y escribe:
   - Â¿QuÃ© tipo de problema es?
   - Â¿QuÃ© features y etiquetas usarÃ­as?
   - Â¿QuÃ© mÃ©trica considerarÃ­as?
2. Define dos riesgos Ã©ticos o de sesgo en ese dataset.
3. Dibuja las curvas de aprendizaje esperadas para underfitting y overfitting; anota cÃ³mo actuarÃ­as en cada caso.
